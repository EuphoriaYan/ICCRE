{
    "gccre_ratio": 0.1,
    "gccre_decay": 0.1,
    "gccre_warmup": 0,
    "bert_frozen": "true",
    "hidden_size": 1536,
    "hidden_dropout_prob": 0.1,
    "classifier_sign": "single_linear",
    "bert_config": {
        "attention_probs_dropout_prob": 0.1,
        "directionality": "bidi",
        "hidden_act": "gelu",
        "hidden_dropout_prob": 0.1,
        "hidden_size": 768,
        "initializer_range": 0.02,
        "intermediate_size": 3072,
        "max_position_embeddings": 512,
        "num_attention_heads": 12,
        "num_hidden_layers": 12,
        "pooler_fc_size": 768,
        "pooler_num_attention_heads": 12,
        "pooler_num_fc_layers": 3,
        "pooler_size_per_head": 128,
        "pooler_type": "first_token_transform",
        "type_vocab_size": 2,
        "vocab_size": 21128
    },
    "gccre_config":{
        "hidden_dropout_prob": 0.3,
        "max_position_embeddings": 512,
        "bert_model": "chinese_wwm_ext_pytorch",
        "gccre_pos_emb_model": "gccre_pos_emb_pytorch",
        "char2word_dim": 0,
        "char_drop": 0.3,
        "char_embsize": 300,
        "cnn_dropout": 0.3,
        "dropout": 0.3,
        "gccre_embsize": 768,
        "fc_merge": true,
        "idx2char": null,
        "level": "char",
        "loss_mask_ids": [0, 101, 102],
        "num_fonts_concat": 1,
        "output_size": 768,
        "use_pretrained_char_embedding": true,
        "pretrained_char_embedding": "",
        "pretrained_char_embedding_path": "sgns.sikuquanshu.word/sgns.sikuquanshu.word",
        "subchar_embsize": 300,
        "subchar_type": "lstm",
        "use_batch_norm": true,
        "use_layer_norm": false,
        "use_maxpool": true,
        "word_embsize": 1024
    },
    "transformer_config": {
        "max_position_embeddings": 512,
        "attention_probs_dropout_prob": 0.5,
        "directionality": "bidi",
        "hidden_act": "gelu",
        "hidden_dropout_prob": 0.5,
        "hidden_size": 1536,
        "initializer_range": 0.02,
        "intermediate_size": 6144,
        "num_attention_heads": 24,
        "num_hidden_layers": 2,
        "pooler_fc_size": 1536,
        "pooler_num_attention_heads": 12,
        "pooler_num_fc_layers": 2,
        "pooler_size_per_head": 256,
        "pooler_type": "first_token_transform",
        "type_vocab_size": 2,
        "vocab_size": 21128
    }
}
